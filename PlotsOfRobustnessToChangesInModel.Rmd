---
title: "RobustnessToChangesInModel"
author: "Rasmus Brostr√∏m"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(stats)
library(boot)
library(plotrix)
library(nlstools)
```

```{r}
#setwd("C:/Projects/Data-driven-algorithms-for-Impulse-control-problem-Thesis")
setwd("C:/Uni/Data-driven-algorithms-for-Impulse-control-problem-Thesis")
data <- read.csv("./SimulationData/DriftAndRewardData/DriftsAndRewards.csv")
```

# Different drift coefficients
First, lets see how the different drift slope coefficients impacted the optimal threshold on average over all reward functions.
```{r}
avgYStartCs <- data %>% group_by(C) %>% summarise(avgOptThreshold = mean(y_star),
                                                  .groups = "drop")

avgYStartCs
```
Here we see that on average the higher the linear coefficient is for the drift the lower the the optimal threshold is, which corresponds with how it affects the expected hitting times. As the linear coefficient increases, the expected hitting times goes from a linear growth to an exponential drift, meaning that on average it takes way too long for the process to get to certain values, and though the maximum reward might be achieved by stopping the process at higher values it isn't beneficial, since the agent is able to stop the process several times and get the same cumulative reward with a lower threshold.

To see the general impact of the drift function on the data-driven algorithm I will look at the impact of the different drift coefficients over all of the reward functions first. Therefore, I group over all the different time horizons and the different coefficients for C.

```{r}
driftAvgData <- data %>% group_by(T, C) %>% 
                          summarise(avgRegret = mean(regret),
                                    seRegret = std.error(regret),
                                    avgST = mean(S_T),
                                    seST = std.error(S_T),
                                    avgDataDecisions = mean(dataStratNrDecisions),
                                    seDataDecisions =std.error(dataStratNrDecisions),
                                    avgOptDecisions = mean(optNrDecisions),
                                    seOptDecisions = std.error(optNrDecisions),
                                    .groups = "drop")

driftAvgData$C = as.factor(driftAvgData$C)
```

Plotting the average regret lines with their 95 % confidence interval based on the standard error.
```{r}
ggplot(driftAvgData, aes(x=T, group=C))+
  geom_line(aes(y=avgRegret, color=C))+
  geom_line(aes(y=avgRegret + 1.96*seRegret))+
  geom_line(aes(y=avgRegret - 1.96*seRegret))
```
Here we see that the higher the linear drift, the higher the average regret is and the regret starts to take on the shape of the upper bound for the cumulative regret, while lower values of the slope coefficient for the linear drift makes the average regret have the form of the lower bound.
The reason for this could have something to do with the fact that when the slope of the drift is small, then the brownian motion has a higher impact. Furthermore, we also see that the higher drift creates more variability to the regrets and that the variability increases with the time horizon, which is expected, since there are more time to create a difference in the rewards and thus create a difference in the regrets.

Lets confirm that the exploration times on average was of order $T^{2/3}$, and look at the confidence bounds aswell, to ensure that there aren't too much variability in the exploration times affecting the regret.
```{r}
ggplot(driftAvgData, aes(x=T, group=C))+
  geom_line(aes(y=avgST))+
  geom_line(aes(y=T^(2/3), color="red"))+
  facet_grid(vars(C))
```
It is clear that exploration times follow the theoretical exploration times very well with small deviations for both $C=0.1$ and $C=4$. Therefore, it does not seem like this should be causing a difference in the average regret.


# Reward functions

## Power of the reward function
```{r}
powerAvgData <- data %>% group_by(T, power) %>% 
                          summarise(avgRegret = mean(regret),
                                    seRegret = std.error(regret),
                                    avgST = mean(S_T),
                                    seST = std.error(S_T),
                                    avgDataDecisions = mean(dataStratNrDecisions),
                                    seDataDecisions =std.error(dataStratNrDecisions),
                                    avgOptDecisions = mean(optNrDecisions),
                                    seOptDecisions = std.error(optNrDecisions),
                                    .groups = "drop")

powerAvgData$power = as.factor(powerAvgData$power)
```

```{r}
ggplot(powerAvgData, aes(x=T, group=power))+
  geom_line(aes(y=avgRegret, color=power))+
  geom_line(aes(y=avgRegret + 1.96*seRegret))+
  geom_line(aes(y=avgRegret - 1.96*seRegret))
```


## Closeness to zero for X=0
```{r}
zeroValAvgData <- data %>% group_by(T, zeroVal) %>% 
                          summarise(avgRegret = mean(regret),
                                    seRegret = std.error(regret),
                                    avgST = mean(S_T),
                                    seST = std.error(S_T),
                                    avgDataDecisions = mean(dataStratNrDecisions),
                                    seDataDecisions =std.error(dataStratNrDecisions),
                                    avgOptDecisions = mean(optNrDecisions),
                                    seOptDecisions = std.error(optNrDecisions),
                                    .groups = "drop")

zeroValAvgData$zeroVal = as.factor(zeroValAvgData$zeroVal)
```

```{r}
ggplot(zeroValAvgData, aes(x=T, group=zeroVal))+
  geom_line(aes(y=avgRegret, color=zeroVal))+
  geom_line(aes(y=avgRegret + 1.96*seRegret))+
  geom_line(aes(y=avgRegret - 1.96*seRegret))
```

# Interaction between the 


# Fit to regret table

```{r}
avgData <- data %>% group_by(T, C, power, zeroVal) %>% summarise(avgRegret = mean(regret),
                                                                 .groups = "drop")

fitData <- avgData %>% group_by(C, power, zeroVal) %>% 
summarise(
  pow=summary(nls(avgRegret ~ I(a*T^p), start=list(p=1, a=1), trace=F))$coefficients[1,1],
  powSE=summary(nls(avgRegret ~ I(a*T^p),start=list(p=1,a=1), trace=F))$coefficients[1,2],
  powSig=summary(nls(avgRegret~I(a*T^p),start=list(p=1,a=1), trace=F))$coefficients[1,4]<0.05,
  a=summary(nls(avgRegret ~ I(a*T^p), start=list(p=1, a=1), trace=F))$coefficients[2,1],
  aSE = summary(nls(avgRegret ~ I(a*T^p), start=list(p=1,a=1), trace=F))$coefficients[2,2],
  aSig = summary(nls(avgRegret~I(a*T^p),start=list(p=1,a=1), trace=F))$coefficients[2,4]<0.05,
  .groups = "drop")

fitData
```


```{r}
test1 <- filter(data, power == 0.5 & C == 0.5 & zeroVal == 0.99)

test2 <- test1 %>% group_by(T) %>% summarise(avgRegret = mean(regret),
                                            .groups = "drop")
ggplot(test2, aes(x=T, y=avgRegret))+
  geom_line()

m <- nls(avgRegret ~ I(a*T^p), data = test2, start = list(p=1, a=1), trace = F)

summary(m)
plotfit(m, smooth = T)
plot(nlsResiduals(m))
```


